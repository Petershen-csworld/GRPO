I1003 01:35:36.162503 140524876534848 train_showo2.py:369]
activation_checkpointing: false
allow_tf32: true
dataset: /mnt/sphere/2025intern/has052/GRPO/FlowGRPO_Showo/dataset/pickscore
eval_freq: 60
fsdp_optimizer_offload: false
logdir: logs
mixed_precision: fp16
num_checkpoint_limit: 5
per_prompt_stat_tracking: true
pretrained:
  model: showlab/show-o2-1.5B
  revision: main
prompt_fn: general_ocr
prompt_fn_kwargs: {}
resolution: 512
reward_fn:
  pickscore: 1.0
run_name: 2025.10.03_01.35.34
sample:
  batch_size: 8
  eval_num_steps: 42
  global_std: true
  guidance_scale: 4.5
  noise_level: 0.7
  num_batches_per_epoch: 8
  num_image_per_prompt: 2
  num_steps: 12
  same_latent: false
  sde_window_range: !!python/tuple
  - 0
  - 10
  sde_window_size: 2
  test_batch_size: 4
  train_batch_size: 2
save_dir: logs/pickscore/sd3.5-M
save_freq: 60
seed: 42
train:
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  adam_weight_decay: 0.0001
  adv_clip_max: 5
  batch_size: 2
  beta: 0.01
  cfg: true
  clip_range: 0.001
  ema: true
  gradient_accumulation_steps: 4
  learning_rate: 0.0003
  lora_path: null
  max_grad_norm: 1.0
  num_inner_epochs: 1
  timestep_fraction: 0.99
  use_8bit_adam: false
use_lora: true

W1003 01:35:36.713659 140524876534848 logging.py:328] Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Number of parameters in diffusion_head_a: 859997440
I1003 01:35:50.498842 140524876534848 modeling_utils.py:971] All model checkpoint weights were used when initializing Showo2Qwen2_5.

I1003 01:35:50.499241 140524876534848 modeling_utils.py:979] All the weights of Showo2Qwen2_5 were initialized from the model checkpoint at showlab/show-o2-1.5B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Showo2Qwen2_5 for predictions without further training.
Number of parameters in diffusion_head_a: 859997440
I1003 01:36:05.200756 140524876534848 modeling_utils.py:971] All model checkpoint weights were used when initializing Showo2Qwen2_5.

I1003 01:36:05.201168 140524876534848 modeling_utils.py:979] All the weights of Showo2Qwen2_5 were initialized from the model checkpoint at showlab/show-o2-1.5B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Showo2Qwen2_5 for predictions without further training.
I1003 01:36:06.648320 140524876534848 wan21_vae.py:615] loading /mnt/sphere/2025intern/has052/show-o2/Wan2.1_VAE.pth
/mnt/sphere/2025intern/has052/GRPO/show-o2/models/wan21_vae.py:617: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(pretrained_path, map_location=device), assign=True)
diffusion_head_a.0.self_attn.q_proj.weight
diffusion_head_a.0.self_attn.k_proj.weight
diffusion_head_a.0.self_attn.v_proj.weight
diffusion_head_a.0.self_attn.o_proj.weight
diffusion_head_a.0.self_attn.q_norm.weight
diffusion_head_a.0.self_attn.k_norm.weight
diffusion_head_a.0.mlp.gate_proj.weight
diffusion_head_a.0.mlp.up_proj.weight
diffusion_head_a.0.mlp.down_proj.weight
diffusion_head_a.0.input_layernorm.weight
diffusion_head_a.0.post_attention_layernorm.weight
diffusion_head_a.0.adaLN_modulation.1.weight
diffusion_head_a.0.adaLN_modulation.1.bias
diffusion_head_a.1.self_attn.q_proj.weight
diffusion_head_a.1.self_attn.k_proj.weight
diffusion_head_a.1.self_attn.v_proj.weight
diffusion_head_a.1.self_attn.o_proj.weight
diffusion_head_a.1.self_attn.q_norm.weight
diffusion_head_a.1.self_attn.k_norm.weight
diffusion_head_a.1.mlp.gate_proj.weight
diffusion_head_a.1.mlp.up_proj.weight
diffusion_head_a.1.mlp.down_proj.weight
diffusion_head_a.1.input_layernorm.weight
diffusion_head_a.1.post_attention_layernorm.weight
diffusion_head_a.1.adaLN_modulation.1.weight
diffusion_head_a.1.adaLN_modulation.1.bias
diffusion_head_a.2.self_attn.q_proj.weight
diffusion_head_a.2.self_attn.k_proj.weight
diffusion_head_a.2.self_attn.v_proj.weight
diffusion_head_a.2.self_attn.o_proj.weight
diffusion_head_a.2.self_attn.q_norm.weight
diffusion_head_a.2.self_attn.k_norm.weight
diffusion_head_a.2.mlp.gate_proj.weight
diffusion_head_a.2.mlp.up_proj.weight
diffusion_head_a.2.mlp.down_proj.weight
diffusion_head_a.2.input_layernorm.weight
diffusion_head_a.2.post_attention_layernorm.weight
diffusion_head_a.2.adaLN_modulation.1.weight
diffusion_head_a.2.adaLN_modulation.1.bias
diffusion_head_a.3.self_attn.q_proj.weight
diffusion_head_a.3.self_attn.k_proj.weight
diffusion_head_a.3.self_attn.v_proj.weight
diffusion_head_a.3.self_attn.o_proj.weight
diffusion_head_a.3.self_attn.q_norm.weight
diffusion_head_a.3.self_attn.k_norm.weight
diffusion_head_a.3.mlp.gate_proj.weight
diffusion_head_a.3.mlp.up_proj.weight
diffusion_head_a.3.mlp.down_proj.weight
diffusion_head_a.3.input_layernorm.weight
diffusion_head_a.3.post_attention_layernorm.weight
diffusion_head_a.3.adaLN_modulation.1.weight
diffusion_head_a.3.adaLN_modulation.1.bias
diffusion_head_a.4.self_attn.q_proj.weight
diffusion_head_a.4.self_attn.k_proj.weight
diffusion_head_a.4.self_attn.v_proj.weight
diffusion_head_a.4.self_attn.o_proj.weight
diffusion_head_a.4.self_attn.q_norm.weight
diffusion_head_a.4.self_attn.k_norm.weight
diffusion_head_a.4.mlp.gate_proj.weight
diffusion_head_a.4.mlp.up_proj.weight
diffusion_head_a.4.mlp.down_proj.weight
diffusion_head_a.4.input_layernorm.weight
diffusion_head_a.4.post_attention_layernorm.weight
diffusion_head_a.4.adaLN_modulation.1.weight
diffusion_head_a.4.adaLN_modulation.1.bias
diffusion_head_a.5.self_attn.q_proj.weight
diffusion_head_a.5.self_attn.k_proj.weight
diffusion_head_a.5.self_attn.v_proj.weight
diffusion_head_a.5.self_attn.o_proj.weight
diffusion_head_a.5.self_attn.q_norm.weight
diffusion_head_a.5.self_attn.k_norm.weight
diffusion_head_a.5.mlp.gate_proj.weight
diffusion_head_a.5.mlp.up_proj.weight
diffusion_head_a.5.mlp.down_proj.weight
diffusion_head_a.5.input_layernorm.weight
diffusion_head_a.5.post_attention_layernorm.weight
diffusion_head_a.5.adaLN_modulation.1.weight
diffusion_head_a.5.adaLN_modulation.1.bias
diffusion_head_a.6.self_attn.q_proj.weight
diffusion_head_a.6.self_attn.k_proj.weight
diffusion_head_a.6.self_attn.v_proj.weight
diffusion_head_a.6.self_attn.o_proj.weight
diffusion_head_a.6.self_attn.q_norm.weight
diffusion_head_a.6.self_attn.k_norm.weight
diffusion_head_a.6.mlp.gate_proj.weight
diffusion_head_a.6.mlp.up_proj.weight
diffusion_head_a.6.mlp.down_proj.weight
diffusion_head_a.6.input_layernorm.weight
diffusion_head_a.6.post_attention_layernorm.weight
diffusion_head_a.6.adaLN_modulation.1.weight
diffusion_head_a.6.adaLN_modulation.1.bias
diffusion_head_a.7.self_attn.q_proj.weight
diffusion_head_a.7.self_attn.k_proj.weight
diffusion_head_a.7.self_attn.v_proj.weight
diffusion_head_a.7.self_attn.o_proj.weight
diffusion_head_a.7.self_attn.q_norm.weight
diffusion_head_a.7.self_attn.k_norm.weight
diffusion_head_a.7.mlp.gate_proj.weight
diffusion_head_a.7.mlp.up_proj.weight
diffusion_head_a.7.mlp.down_proj.weight
diffusion_head_a.7.input_layernorm.weight
diffusion_head_a.7.post_attention_layernorm.weight
diffusion_head_a.7.adaLN_modulation.1.weight
diffusion_head_a.7.adaLN_modulation.1.bias
diffusion_head_a.8.self_attn.q_proj.weight
diffusion_head_a.8.self_attn.k_proj.weight
diffusion_head_a.8.self_attn.v_proj.weight
diffusion_head_a.8.self_attn.o_proj.weight
diffusion_head_a.8.self_attn.q_norm.weight
diffusion_head_a.8.self_attn.k_norm.weight
diffusion_head_a.8.mlp.gate_proj.weight
diffusion_head_a.8.mlp.up_proj.weight
diffusion_head_a.8.mlp.down_proj.weight
diffusion_head_a.8.input_layernorm.weight
diffusion_head_a.8.post_attention_layernorm.weight
diffusion_head_a.8.adaLN_modulation.1.weight
diffusion_head_a.8.adaLN_modulation.1.bias
diffusion_head_a.9.self_attn.q_proj.weight
diffusion_head_a.9.self_attn.k_proj.weight
diffusion_head_a.9.self_attn.v_proj.weight
diffusion_head_a.9.self_attn.o_proj.weight
diffusion_head_a.9.self_attn.q_norm.weight
diffusion_head_a.9.self_attn.k_norm.weight
diffusion_head_a.9.mlp.gate_proj.weight
diffusion_head_a.9.mlp.up_proj.weight
diffusion_head_a.9.mlp.down_proj.weight
diffusion_head_a.9.input_layernorm.weight
diffusion_head_a.9.post_attention_layernorm.weight
diffusion_head_a.9.adaLN_modulation.1.weight
diffusion_head_a.9.adaLN_modulation.1.bias
diffusion_head_b.norm_final.weight
diffusion_head_b.linear.weight
diffusion_head_b.linear.bias
diffusion_head_b.adaLN_modulation.1.weight
diffusion_head_b.adaLN_modulation.1.bias
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
per prompt stat tracking True
I1003 01:36:12.708462 140524876534848 train_showo2.py:561] ***** Running training *****
I1003 01:36:12.708674 140524876534848 train_showo2.py:562]   Sample batch size per device = 2
I1003 01:36:12.708789 140524876534848 train_showo2.py:563]   Train batch size per device = 2
I1003 01:36:12.708895 140524876534848 train_showo2.py:564]   Gradient Accumulation steps = 4
I1003 01:36:12.709018 140524876534848 train_showo2.py:567]
I1003 01:36:12.709121 140524876534848 train_showo2.py:568]   Total number of samples per epoch = 16
I1003 01:36:12.709222 140524876534848 train_showo2.py:569]   Total train batch size (w. parallel, distributed & accumulation) = 8
I1003 01:36:12.709332 140524876534848 train_showo2.py:572]   Number of gradient updates per inner epoch = 2
I1003 01:36:12.709444 140524876534848 train_showo2.py:575]   Number of inner epochs = 1
Eval:   0%|       W1003 01:36:16.964051 140524876534848 logging.py:328] The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
prompts ['Beautiful woman in skin tight body suite working out in the gymq', 'a jung male cyborg with white hair sitting down on a throne in a dystopian world, digital art, epic', 'beautiful asian girl with huge naturals in pool', 'man having fun with his girl in his bed']
40it [00:32,  1.23it/s]
/mnt/sphere/2025intern/has052/GRPO/show-o2/models/wan21_vae.py:651: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(dtype=self.dtype):
40it [00:30,  1.33it/s]                                      | 1/512 [00:34<4:53:48, 34.50s/it]
prompts ['enigmatic black square building on top of a purple hill, smoke stacks', 'Noah is standing with his wife and three sons on ark.the ark is  floating on Ocean Realistic Image', 'genere un perfil de rostro de 3/4 de una mujer sofisticada de 23 años latina, alegre, cabello negro', 'an epic angel dressed in blue with white wings']
40it [00:30,  1.32it/s]                                      | 2/512 [01:04<4:33:13, 32.14s/it]
prompts ['cinematic still of highly reflective stainless steel Bitcoin logo in the desert, at sunset', 'wide shot of detailed compact simple starship in space, battle, fast, simple volume, geometrical, star wars, sharp, dynamic, cyberpunk, revolutionary, lightweight, volvo, bertone, minecraft, funky, studio scene, high contrast, super realistic, volumetric lighting, glimmery light, product lighting, high detail, super realistic, high contrast, cinematic lighting, rtx, octane render, behance, 4k, highly detailed, professional photograph, terrazzo, advertisement, epic', 'photo of Ford Focus RS, night time, city, city roads, Miami streets', 'An impressionist portrait of a shar pei']
14it [00:11,  1.24it/s]                                      | 3/512 [01:35<4:26:59, 31.47s/it]
prompts ['Labrador in anime style, black and white photography, highly detailed,', 'eating pizza at a romantic date', 'Davy Crockett eating a burger at McDonalds', 'exquisite marble detail, spray, glitter, holding battery powered dildo, twisted, wacky, skinny Latino cappuccino nerd, dork girl wearing tiny shorts, doing full body twisted splits breakdance, upside down bare model, smoke, explosion, 8K, HD, magical energy, highly detailed, rendered in octane, very very very aesthetic,']
14it [00:10,  1.32it/s]
