I1003 01:33:43.275627 139740010853440 train_showo2.py:369]
activation_checkpointing: false
allow_tf32: true
dataset: /mnt/sphere/2025intern/has052/GRPO/FlowGRPO_Showo/dataset/pickscore
eval_freq: 60
fsdp_optimizer_offload: false
logdir: logs
mixed_precision: fp16
num_checkpoint_limit: 5
per_prompt_stat_tracking: true
pretrained:
  model: showlab/show-o2-1.5B
  revision: main
prompt_fn: general_ocr
prompt_fn_kwargs: {}
resolution: 512
reward_fn:
  pickscore: 1.0
run_name: 2025.10.03_01.33.41
sample:
  batch_size: 8
  eval_num_steps: 42
  global_std: true
  guidance_scale: 4.5
  noise_level: 0.7
  num_batches_per_epoch: 32
  num_image_per_prompt: 4
  num_steps: 12
  same_latent: false
  sde_window_range: !!python/tuple
  - 0
  - 10
  sde_window_size: 2
  test_batch_size: 4
  train_batch_size: 1
save_dir: logs/pickscore/sd3.5-M
save_freq: 60
seed: 42
train:
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  adam_weight_decay: 0.0001
  adv_clip_max: 5
  batch_size: 1
  beta: 0.01
  cfg: true
  clip_range: 0.001
  ema: true
  gradient_accumulation_steps: 16
  learning_rate: 0.0003
  lora_path: null
  max_grad_norm: 1.0
  num_inner_epochs: 1
  timestep_fraction: 0.99
  use_8bit_adam: false
use_lora: true

W1003 01:33:43.956775 139740010853440 logging.py:328] Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Number of parameters in diffusion_head_a: 859997440
I1003 01:33:57.279561 139740010853440 modeling_utils.py:971] All model checkpoint weights were used when initializing Showo2Qwen2_5.

I1003 01:33:57.279989 139740010853440 modeling_utils.py:979] All the weights of Showo2Qwen2_5 were initialized from the model checkpoint at showlab/show-o2-1.5B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Showo2Qwen2_5 for predictions without further training.
Number of parameters in diffusion_head_a: 859997440
I1003 01:34:11.710384 139740010853440 modeling_utils.py:971] All model checkpoint weights were used when initializing Showo2Qwen2_5.

I1003 01:34:11.710810 139740010853440 modeling_utils.py:979] All the weights of Showo2Qwen2_5 were initialized from the model checkpoint at showlab/show-o2-1.5B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Showo2Qwen2_5 for predictions without further training.
I1003 01:34:13.028449 139740010853440 wan21_vae.py:615] loading /mnt/sphere/2025intern/has052/show-o2/Wan2.1_VAE.pth
/mnt/sphere/2025intern/has052/GRPO/show-o2/models/wan21_vae.py:617: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(pretrained_path, map_location=device), assign=True)
diffusion_head_a.0.self_attn.q_proj.weight
diffusion_head_a.0.self_attn.k_proj.weight
diffusion_head_a.0.self_attn.v_proj.weight
diffusion_head_a.0.self_attn.o_proj.weight
diffusion_head_a.0.self_attn.q_norm.weight
diffusion_head_a.0.self_attn.k_norm.weight
diffusion_head_a.0.mlp.gate_proj.weight
diffusion_head_a.0.mlp.up_proj.weight
diffusion_head_a.0.mlp.down_proj.weight
diffusion_head_a.0.input_layernorm.weight
diffusion_head_a.0.post_attention_layernorm.weight
diffusion_head_a.0.adaLN_modulation.1.weight
diffusion_head_a.0.adaLN_modulation.1.bias
diffusion_head_a.1.self_attn.q_proj.weight
diffusion_head_a.1.self_attn.k_proj.weight
diffusion_head_a.1.self_attn.v_proj.weight
diffusion_head_a.1.self_attn.o_proj.weight
diffusion_head_a.1.self_attn.q_norm.weight
diffusion_head_a.1.self_attn.k_norm.weight
diffusion_head_a.1.mlp.gate_proj.weight
diffusion_head_a.1.mlp.up_proj.weight
diffusion_head_a.1.mlp.down_proj.weight
diffusion_head_a.1.input_layernorm.weight
diffusion_head_a.1.post_attention_layernorm.weight
diffusion_head_a.1.adaLN_modulation.1.weight
diffusion_head_a.1.adaLN_modulation.1.bias
diffusion_head_a.2.self_attn.q_proj.weight
diffusion_head_a.2.self_attn.k_proj.weight
diffusion_head_a.2.self_attn.v_proj.weight
diffusion_head_a.2.self_attn.o_proj.weight
diffusion_head_a.2.self_attn.q_norm.weight
diffusion_head_a.2.self_attn.k_norm.weight
diffusion_head_a.2.mlp.gate_proj.weight
diffusion_head_a.2.mlp.up_proj.weight
diffusion_head_a.2.mlp.down_proj.weight
diffusion_head_a.2.input_layernorm.weight
diffusion_head_a.2.post_attention_layernorm.weight
diffusion_head_a.2.adaLN_modulation.1.weight
diffusion_head_a.2.adaLN_modulation.1.bias
diffusion_head_a.3.self_attn.q_proj.weight
diffusion_head_a.3.self_attn.k_proj.weight
diffusion_head_a.3.self_attn.v_proj.weight
diffusion_head_a.3.self_attn.o_proj.weight
diffusion_head_a.3.self_attn.q_norm.weight
diffusion_head_a.3.self_attn.k_norm.weight
diffusion_head_a.3.mlp.gate_proj.weight
diffusion_head_a.3.mlp.up_proj.weight
diffusion_head_a.3.mlp.down_proj.weight
diffusion_head_a.3.input_layernorm.weight
diffusion_head_a.3.post_attention_layernorm.weight
diffusion_head_a.3.adaLN_modulation.1.weight
diffusion_head_a.3.adaLN_modulation.1.bias
diffusion_head_a.4.self_attn.q_proj.weight
diffusion_head_a.4.self_attn.k_proj.weight
diffusion_head_a.4.self_attn.v_proj.weight
diffusion_head_a.4.self_attn.o_proj.weight
diffusion_head_a.4.self_attn.q_norm.weight
diffusion_head_a.4.self_attn.k_norm.weight
diffusion_head_a.4.mlp.gate_proj.weight
diffusion_head_a.4.mlp.up_proj.weight
diffusion_head_a.4.mlp.down_proj.weight
diffusion_head_a.4.input_layernorm.weight
diffusion_head_a.4.post_attention_layernorm.weight
diffusion_head_a.4.adaLN_modulation.1.weight
diffusion_head_a.4.adaLN_modulation.1.bias
diffusion_head_a.5.self_attn.q_proj.weight
diffusion_head_a.5.self_attn.k_proj.weight
diffusion_head_a.5.self_attn.v_proj.weight
diffusion_head_a.5.self_attn.o_proj.weight
diffusion_head_a.5.self_attn.q_norm.weight
diffusion_head_a.5.self_attn.k_norm.weight
diffusion_head_a.5.mlp.gate_proj.weight
diffusion_head_a.5.mlp.up_proj.weight
diffusion_head_a.5.mlp.down_proj.weight
diffusion_head_a.5.input_layernorm.weight
diffusion_head_a.5.post_attention_layernorm.weight
diffusion_head_a.5.adaLN_modulation.1.weight
diffusion_head_a.5.adaLN_modulation.1.bias
diffusion_head_a.6.self_attn.q_proj.weight
diffusion_head_a.6.self_attn.k_proj.weight
diffusion_head_a.6.self_attn.v_proj.weight
diffusion_head_a.6.self_attn.o_proj.weight
diffusion_head_a.6.self_attn.q_norm.weight
diffusion_head_a.6.self_attn.k_norm.weight
diffusion_head_a.6.mlp.gate_proj.weight
diffusion_head_a.6.mlp.up_proj.weight
diffusion_head_a.6.mlp.down_proj.weight
diffusion_head_a.6.input_layernorm.weight
diffusion_head_a.6.post_attention_layernorm.weight
diffusion_head_a.6.adaLN_modulation.1.weight
diffusion_head_a.6.adaLN_modulation.1.bias
diffusion_head_a.7.self_attn.q_proj.weight
diffusion_head_a.7.self_attn.k_proj.weight
diffusion_head_a.7.self_attn.v_proj.weight
diffusion_head_a.7.self_attn.o_proj.weight
diffusion_head_a.7.self_attn.q_norm.weight
diffusion_head_a.7.self_attn.k_norm.weight
diffusion_head_a.7.mlp.gate_proj.weight
diffusion_head_a.7.mlp.up_proj.weight
diffusion_head_a.7.mlp.down_proj.weight
diffusion_head_a.7.input_layernorm.weight
diffusion_head_a.7.post_attention_layernorm.weight
diffusion_head_a.7.adaLN_modulation.1.weight
diffusion_head_a.7.adaLN_modulation.1.bias
diffusion_head_a.8.self_attn.q_proj.weight
diffusion_head_a.8.self_attn.k_proj.weight
diffusion_head_a.8.self_attn.v_proj.weight
diffusion_head_a.8.self_attn.o_proj.weight
diffusion_head_a.8.self_attn.q_norm.weight
diffusion_head_a.8.self_attn.k_norm.weight
diffusion_head_a.8.mlp.gate_proj.weight
diffusion_head_a.8.mlp.up_proj.weight
diffusion_head_a.8.mlp.down_proj.weight
diffusion_head_a.8.input_layernorm.weight
diffusion_head_a.8.post_attention_layernorm.weight
diffusion_head_a.8.adaLN_modulation.1.weight
diffusion_head_a.8.adaLN_modulation.1.bias
diffusion_head_a.9.self_attn.q_proj.weight
diffusion_head_a.9.self_attn.k_proj.weight
diffusion_head_a.9.self_attn.v_proj.weight
diffusion_head_a.9.self_attn.o_proj.weight
diffusion_head_a.9.self_attn.q_norm.weight
diffusion_head_a.9.self_attn.k_norm.weight
diffusion_head_a.9.mlp.gate_proj.weight
diffusion_head_a.9.mlp.up_proj.weight
diffusion_head_a.9.mlp.down_proj.weight
diffusion_head_a.9.input_layernorm.weight
diffusion_head_a.9.post_attention_layernorm.weight
diffusion_head_a.9.adaLN_modulation.1.weight
diffusion_head_a.9.adaLN_modulation.1.bias
diffusion_head_b.norm_final.weight
diffusion_head_b.linear.weight
diffusion_head_b.linear.bias
diffusion_head_b.adaLN_modulation.1.weight
diffusion_head_b.adaLN_modulation.1.bias
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Traceback (most recent call last):
  File "/mnt/sphere/2025intern/has052/GRPO/FlowGRPO_Showo/./train_showo2.py", line 971, in <module>
    app.run(main)
  File "/home/has052/miniconda3/envs/showo/lib/python3.10/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/home/has052/miniconda3/envs/showo/lib/python3.10/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))
  File "/mnt/sphere/2025intern/has052/GRPO/FlowGRPO_Showo/./train_showo2.py", line 461, in main
    train_sampler = DistributedKRepeatSampler(
  File "/mnt/sphere/2025intern/has052/GRPO/FlowGRPO_Showo/./train_showo2.py", line 96, in __init__
    assert self.total_samples % self.k == 0, f"k can not divide n*b, k{k}-num_replicas{num_replicas}-batch_size{batch_size}"
AssertionError: k can not divide n*b, k4-num_replicas1-batch_size1
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/sphere/2025intern/has052/GRPO/FlowGRPO_Showo/./train_showo2.py", line 971, in <module>
[rank0]:     app.run(main)
[rank0]:   File "/home/has052/miniconda3/envs/showo/lib/python3.10/site-packages/absl/app.py", line 316, in run
[rank0]:     _run_main(main, args)
[rank0]:   File "/home/has052/miniconda3/envs/showo/lib/python3.10/site-packages/absl/app.py", line 261, in _run_main
[rank0]:     sys.exit(main(argv))
[rank0]:   File "/mnt/sphere/2025intern/has052/GRPO/FlowGRPO_Showo/./train_showo2.py", line 461, in main
[rank0]:     train_sampler = DistributedKRepeatSampler(
[rank0]:   File "/mnt/sphere/2025intern/has052/GRPO/FlowGRPO_Showo/./train_showo2.py", line 96, in __init__
[rank0]:     assert self.total_samples % self.k == 0, f"k can not divide n*b, k{k}-num_replicas{num_replicas}-batch_size{batch_size}"
[rank0]: AssertionError: k can not divide n*b, k4-num_replicas1-batch_size1
